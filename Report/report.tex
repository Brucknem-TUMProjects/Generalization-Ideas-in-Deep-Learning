\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=3cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{seminar}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{float}
\usepackage{hyperref}
\usepackage{mathtools}

\pagestyle{fancy}

\graphicspath{ {../Presentation/}}

%You can add theorem-like environments (e.g. remark, definition, ...) if you want
\newtheorem{theorem}{Theorem}

\title{Generalization Ideas in Deep Learning} % Replace with your title
\author{Marcel Bruckner} % Replace with your name
\institute{\textit{Seminar: Optimization and Generalization in Deep Learning}}

\makeatletter
\let\runauthor\@author
\let\runtitle\@title
\makeatother
\lhead{\runauthor}
\rhead{\runtitle}

\newcommand{\signed}[1]{%
	{\unskip\nobreak\hfil\penalty50
		\hskip2em\hbox{}\nobreak\hfil#1
		\parfillskip=0pt \finalhyphendemerits=0 \par}}
\newcommand\inlineeqno{\signed{\stepcounter{equation}\ (\theequation)}}
\newcommand{\fref}[1]{(Fig:~\ref{#1})}
\newcommand{\freff}[1]{Figure~\ref{#1}}
\newcommand{\sref}[1]{Section~\ref{#1}}
\newcommand{\ns}{Neyshabur~et~al.~\cite{neyshabur2017exploring}}

\begin{document}

\maketitle

\begin{abstract}
Trying to understand the generalization abilities of deep neural networks several capacity measures are experimentally explored. Following the thoughts of \ns{} different norm-based capacity measures on the network weights and sharpness as the robustness to perturbations on the parameter space are investigated. The measures are used in different experiments and the results are plotted against each other.
\end{abstract}

\section{Introduction}
In recent years we have seen the huge empirical success of deep neural networks. Even though they solve complex, highly non-convex optimization problems simple stochastic gradient descent methods achieve good generalization behavior in practice. We see that even in the highly over parameterized regime in which deep neural networks are located many of the local minima provide good generalization on unseen data. Nonetheless, it remains mostly unclear why they exhibit the desired generalization behavior.\\
%
\ns{} provided us with a methodology to investigate into measures of the generalization ability. Using this framework we tried to reproduce their results and to understand how different measures on the parameter space can explain certain empirical phenomena that are directly connected to generalization abilities of deep neural networks.
%
\input{capacity_measures.tex}

\input{empirical_study.tex}

\input{conclusion.tex}

\newpage

\input{appendix.tex}

\newpage

\nocite{*}

\bibliographystyle{plain}
\bibliography{egbib}



\end{document}
